# Generic, Reusable AI Agent Architecture

This document outlines a generic, reusable architecture for building AI agents, leveraging modern frameworks like AG-UI, FastMCP, `mcp-use`, LangGraph, and LightRAG. The goal is to create a modular, scalable, and maintainable system.

## Guiding Principles

*   **Modularity**: Each component should be independent and replaceable.
*   **Reusability**: Components (especially tools and orchestration logic) should be easily reusable across different agents.
*   **Standardization**: Adhere to protocols like AG-UI for frontend-backend communication and MCP for agent-tool interaction.
*   **Scalability**: Design components to be scalable independently.
*   **Separation of Concerns**: Clearly delineate responsibilities between layers (presentation, gateway, orchestration, tooling).

## Architectural Layers

```mermaid
graph TD
    A[AG-UI Frontend] <--> B{Agent Gateway / BFF API};
    B <--> C{Agent Orchestration Layer (LangGraph + mcp-use + LLM)};
    C --> D[FastMCP Tool Servers];
    C --> E[LightRAG Knowledge Service];
    C --> A; %% For client-side actions / HITL responses

    subgraph Presentation Layer
        A
    end

    subgraph Backend Services
        B
        C
        D
        E
    end

    style A fill:#f9f,stroke:#333,stroke-width:2px
    style B fill:#ccf,stroke:#333,stroke-width:2px
    style C fill:#cfc,stroke:#333,stroke-width:2px
    style D fill:#ff9,stroke:#333,stroke-width:2px
    style E fill:#fca,stroke:#333,stroke-width:2px
```

### 1. Presentation Layer (Frontend)

*   **Technology**: **AG-UI** (e.g., `@copilotkit/react-core`, `@copilotkit/react-ui`).
*   **Responsibilities**:
    *   User interaction (chat, forms, dynamic UI elements).
    *   Rendering agent responses (text, structured data, UI generated by tools).
    *   Implementing client-side "actions" (`useCopilotAction`) callable by the backend.
    *   Handling AG-UI event streaming.
    *   Connecting to the Agent Gateway (`runtimeUrl`).
    *   Providing UI elements for selecting the desired LLM (e.g., OpenAI, Gemini, local Ollama models) and communicating this choice to the backend.
    *   Displaying conversation history and offering controls for chat session management (e.g., initiating a new chat, clearing/restarting the current chat), typically by managing or requesting new `threadId`s.

### 2. Agent Gateway / Backend-for-Frontend (BFF)

*   **Technology**: Lightweight web server (FastAPI, Flask, Next.js/Express API routes).
*   **Responsibilities**:
    *   Expose AG-UI compliant API endpoint(s).
    *   Handle authentication, authorization, and session management.
    *   Act as a bridge, routing requests to the Agent Orchestration Layer and streaming responses back.
    *   Receiving the selected LLM preference from the frontend and passing it to the Agent Orchestration Layer.
    *   Interpreting session management cues from the frontend (e.g., requests for a new chat) to ensure appropriate `threadId` handling when relaying to the Agent Orchestration Layer.

### 3. Agent Orchestration Layer (The "Brain")

*   **Technology**: **LangGraph** (Python) for core logic, **`langchain-mcp-adapters`** (Python) for MCP tool consumption (using `MultiServerMCPClient`), integrated with LLMs (via LangChain or direct SDKs).
*   **Responsibilities**:
    *   Define agent control flow (StateGraphs in LangGraph).
    *   Manage conversational state (compatible with AG-UI, e.g., `CopilotKitState`).
    *   Interface with LLMs for reasoning and decision-making.
    *   Consume tools from FastMCP servers by loading them as LangChain tools via `langchain-mcp-adapters`.
    *   Query LightRAG for knowledge retrieval.
    *   Invoke client-side actions on the AG-UI frontend.
    *   Implement Human-in-the-Loop (HITL) patterns.
    *   Dynamically initializing or selecting the appropriate LLM client (e.g., LangChain's `ChatOpenAI`, `ChatGoogleGenerativeAI`, `ChatOllama`) based on the preference passed from the Agent Gateway.
    *   Reliably managing chat history and state using LangGraph's checkpointer mechanism (e.g., `MemorySaver`), keyed by `thread_id` to support distinct conversations.
    *   Supporting the creation of new chat sessions by operating on new `thread_id`s, effectively isolating conversation histories.

### 4. Tooling & Knowledge Layer (Backend Services)

*   **A. FastMCP Tool Servers**:
    *   **Technology**: **FastMCP** (Python).
    *   **Responsibilities**: Expose discrete, reusable functionalities as MCP tools (e.g., calculations, API integrations, data processing).
*   **B. LightRAG Knowledge Service**:
    *   **Technology**: **LightRAG**.
    *   **Responsibilities**: Document ingestion, embedding, indexing, and providing a retrieval API for semantic search.
*   **C. Other Custom Backend Services**: Databases, legacy systems, etc. (preferably exposed via FastMCP tools).

## Proposed Folder Structure

```
/project-root
├── frontend/                     # AG-UI based frontend application (e.g., Next.js, React)
│   ├── src/
│   │   ├── app/                  # Next.js app router or similar
│   │   │   └── api/
│   │   │       └── copilotkit/   # AG-UI Agent Gateway endpoint (BFF)
│   │   │           └── route.ts  # Example for Next.js
│   │   ├── components/           # Reusable UI components
│   │   ├── agents/               # Client-side agent configurations or helpers
│   │   └── ...                   # Other frontend assets, styles, utils
│   └── public/
├── backend/
│   ├── orchestrators/            # LangGraph agent orchestrators
│   │   └── specific_agent_orchestrator/
│   │       ├── graph.py          # LangGraph definition
│   │       ├── state.py          # Agent state models
│   │       └── prompts.py        # Prompts for the agent
│   ├── mcp_servers/              # FastMCP tool servers
│   │   └── specific_tool_server/
│   │       ├── server.py         # FastMCP server definition
│   │       └── tools/
│   │           └── some_tool.py
│   ├── knowledge_services/       # LightRAG or other knowledge services
│   │   └── lightrag_service/
│   │       └── ...               # LightRAG setup and API exposure
│   ├── core_libs/                # Shared Python libraries for backend services
│   │   └── utils.py
│   └── main_gateway.py           # Optional: A central Python AG-UI gateway if not using Next.js BFF
├── docs/                         # Project documentation
├── scripts/                      # Utility scripts (deployment, testing)
├── tests/
│   ├── frontend/
│   └── backend/
├── .env                          # Environment variables
├── requirements.txt              # Python backend dependencies
├── package.json                  # Frontend dependencies
└── README.md
```

**Meaning of Key Folders:**

*   `frontend/`: All client-facing code. The `api/copilotkit` subfolder is a common pattern for implementing the AG-UI BFF within a Next.js app.
*   `backend/`: All server-side logic and services.
    *   `orchestrators/`: Houses the core logic for different AI agents, built with LangGraph.
    *   `mcp_servers/`: Contains various FastMCP servers, each providing a set of related tools.
    *   `knowledge_services/`: For RAG implementations like LightRAG.
    *   `core_libs/`: Shared Python utilities for backend components.
*   `docs/`: Documentation for the project, architecture, APIs, etc.
*   `scripts/`: Helper scripts for development, deployment, etc.
*   `tests/`: Unit, integration, and end-to-end tests.

## Mini Example: "Echo & UpperCase" Agent

This example demonstrates a very simple agent that can echo messages or convert them to uppercase using a FastMCP tool.

**1. Frontend (`frontend/src/app/page.tsx`) - Conceptual AG-UI**

```tsx
// frontend/src/app/page.tsx (Conceptual - using CopilotKit)
"use client";
import { CopilotKit } from "@copilotkit/react-core";
import { CopilotChat } from "@copilotkit/react-ui";
import "@copilotkit/react-ui/styles.css";

export default function HomePage() {
  return (
    <CopilotKit runtimeUrl="/api/copilotkit"> {/* Points to our BFF */}
      <CopilotChat 
        labels={{
          title: "Echo & UpperCase Agent",
          initial: "Hi! I can echo your messages or make them uppercase. Try 'echo hello' or 'upper hello world'."
        }}
        agent="echo_upper_agent" // Identifies which orchestrator to use
      />
    </CopilotKit>
  );
}
```

**2. Agent Gateway / BFF (`frontend/src/app/api/copilotkit/route.ts`) - Conceptual Next.js**

```typescript
// frontend/src/app/api/copilotkit/route.ts (Conceptual Next.js API route)
// This would typically forward requests to a Python backend running LangGraph.
// For a direct Python AG-UI server, this BFF layer might be simpler or part of the Python service.

import { CopilotRuntime } from "@copilotkit/runtime";

export async function POST(req: Request): Promise<Response> {
  const copilotKit = new CopilotRuntime();
  // This example is simplified. In a real scenario, this endpoint would
  // proxy requests to your Python LangGraph agent service.
  // For instance, by making an HTTP request to the Python service that
  // implements the AG-UI server protocol for the specific agent.
  const agentId = req.headers.get("X-CopilotKit-Agent-Id") || "echo_upper_agent";
  
  // Placeholder: In reality, you'd call your Python backend here.
  // e.g., fetch(`http://localhost:8000/agents/${agentId}`, { method: "POST", body: req.body ... })
  // The Python backend would then handle the AG-UI protocol.
  
  // For this conceptual example, we'll assume a direct handling (not realistic for LangGraph in Python from here)
  return copilotKit.response(req, async (event) => {
    // This part is illustrative of what the Python backend would do.
    if (event.type === "event" && event.name === "copilotkitRequest") {
        // Forward to Python LangGraph service
        console.log("Forwarding to Python backend with payload:", event.payload);
        // The Python service would then stream back AG-UI events.
    }
  });
}

// Note: A more robust solution for Python backends is to have the Python service
// directly implement the AG-UI server capabilities (e.g., using FastAPI and a Python AG-UI server library if available,
// or by manually implementing the AG-UI SSE protocol).
```

**3. FastMCP Tool Server (`backend/mcp_servers/string_tools/server.py`)**

```python
# backend/mcp_servers/string_tools/server.py
from fastmcp import FastMCP

mcp = FastMCP(name="StringTools")

@mcp.tool()
def to_upper_case(text: str) -> str:
    """Converts text to uppercase."""
    return text.upper()

if __name__ == "__main__":
    # To run: python backend/mcp_servers/string_tools/server.py
    # By default, runs on http://localhost:8000 if mcp-use is configured for HTTP
    # or can be run as a subprocess by mcp-use.
    mcp.run()
```

**4. Agent Orchestrator (`backend/orchestrators/echo_upper_orchestrator/graph.py`)**

```python
# backend/main_orchestrator.py
# This would be your main Python file for the agent's brain.

from typing import TypedDict, Annotated, Sequence, List
import operator # For Annotated sequence addition

from langchain_core.messages import AIMessage, BaseMessage, HumanMessage, ToolMessage, ToolCall
from langchain_core.pydantic_v1 import BaseModel
from langgraph.graph import StateGraph, START, END
from langgraph.checkpoint.memory import MemorySaver
from langgraph.prebuilt import ToolNode

from langchain_mcp_adapters.client import MultiServerMCPClient

# --- MCP Client Setup (using langchain-mcp-adapters) ---
mcp_client = MultiServerMCPClient({
    "string_tools": {
        "command": ["python3", "./backend/mcp_servers/string_tools/server.py"], # Adjust path as needed
        "transport": "stdio",
        # Example for an independently running server:
        # "url": "http://localhost:8001/mcp", # Ensure port is correct
        # "transport": "streamable_http",
    }
})

# --- LangGraph State Definition ---
class AgentState(TypedDict):
    messages: Annotated[Sequence[BaseMessage], operator.add]

# --- LangGraph Nodes ---

# Fetches tools from the MCP server using langchain-mcp-adapters
async def get_mcp_tools():
    return await mcp_client.get_tools()

# Mock LLM: Simulates LLM behavior for the mini-example.
# A real LLM (e.g., ChatOpenAI) would be bound to the tools.
async def call_model_mock(state: AgentState):
    current_messages = state["messages"]
    last_message = current_messages[-1]
    
    if isinstance(last_message, ToolMessage):
        # If the last message is a ToolMessage, a tool was just run.
        # A real agent might feed this back to the LLM or decide the next step.
        # For this mock, we'll consider the tool's output as a final part of the response.
        return {"messages": [AIMessage(content=f"Tool '{last_message.name}' executed. Result: {last_message.content}")]}

    if not isinstance(last_message, HumanMessage):
        return {"messages": [AIMessage(content="Error: Mock agent expects HumanMessage or ToolMessage as last message.")]}

    content = last_message.content.lower()
    tool_calls: List[ToolCall] = []

    if content.startswith("echo "):
        response_text = content[5:]
        return {"messages": [AIMessage(content=response_text)]}
    elif content.startswith("upper "):
        text_to_upper = content[6:]
        # Simulate LLM deciding to call the 'to_upper_case' tool.
        # Tool name from FastMCP server 'to_upper_case' on server 'string_tools'
        # becomes 'string_tools_to_upper_case' in LangChain's tool list.
        tool_calls.append(
            ToolCall(name="string_tools_to_upper_case",
                     args={"text": text_to_upper},
                     id="tool_call_upper_123")
        )
        return {"messages": [AIMessage(content="", tool_calls=tool_calls)]}
    else:
        response_text = "Sorry, I can only 'echo <text>' or 'upper <text>'."
        return {"messages": [AIMessage(content=response_text)]}

# --- LangGraph Definition (Async to fetch tools) ---
async def create_echo_upper_agent_graph():
    tools = await get_mcp_tools() # Get LangChain-compatible tools
    tool_node = ToolNode(tools)   # Node to execute these tools

    workflow = StateGraph(AgentState)
    workflow.add_node("agent", call_model_mock) # Mock LLM node
    workflow.add_node("tools", tool_node)       # Tool execution node

    workflow.set_entry_point("agent")

    # Conditional edge: after 'agent' node, decide if to call 'tools' or end.
    def should_invoke_tools(state: AgentState):
        last_message = state["messages"][-1]
        if isinstance(last_message, AIMessage) and hasattr(last_message, "tool_calls") and last_message.tool_calls:
            return "tools" # Route to tool_node if AIMessage has tool_calls
        return END         # Otherwise, end the execution

    workflow.add_conditional_edges("agent", should_invoke_tools)
    
    # After tools are executed, route back to the 'agent' node to process tool results.
    workflow.add_edge("tools", "agent")
    
    # --- Compile Graph ---
    # This is the agent graph that the AG-UI server (Python part) would invoke.
    return workflow.compile(checkpointer=MemorySaver())

# Note: The graph is now created by calling `await create_echo_upper_agent_graph()`.
# The actual compiled graph is not a global variable here but would be obtained 
# when the server starts or handles a request, as shown in `handle_ag_ui_request`.

# --- AG-UI Server (Conceptual - to be run by your Python AG-UI Gateway) ---
# To make this runnable with AG-UI, you'd need a Python server (e.g., FastAPI)
# that implements the AG-UI protocol (handles SSE, message formats) and invokes
# this `echo_upper_agent_graph.astream_events(...)` for each user session/request.
# Example of how it might be invoked (highly simplified):
# This part would typically be in your AG-UI Python Gateway/Server (e.g., a FastAPI endpoint).

# Global variable to hold the compiled graph (initialized once upon first request or server start)
echo_upper_agent_graph_compiled = None

async def get_compiled_graph():
    global echo_upper_agent_graph_compiled
    if echo_upper_agent_graph_compiled is None:
        # This is where the graph is created and compiled, only once.
        echo_upper_agent_graph_compiled = await create_echo_upper_agent_graph()
    return echo_upper_agent_graph_compiled

async def handle_ag_ui_request(payload):
    graph = await get_compiled_graph() # Get or create the compiled graph

    # 'payload' would contain user message, thread_id, etc. from AG-UI
    # Ensure robust access to payload elements
    user_message_content = payload.get("message", {}).get("content", "")
    thread_id = payload.get("threadId", "default_thread")

    config = {"configurable": {"thread_id": thread_id}}
    initial_state = {"messages": [HumanMessage(content=user_message_content)]}
    
    async for event in graph.astream_events(initial_state, config=config, version="v2"):
        # Map LangGraph events to AG-UI SSE events and stream them back.
        # This is a simplified representation; a real AG-UI server would format these events.
        print(f"AGENT EVENT: {event['event']}", event.get('data', {})) # Defensive get for data
        
        # Example: Forwarding AIMessage content as AG-UI text messages
        # A more complete implementation would map various LangGraph events to AG-UI events.
        if event['event'] == 'on_chain_end': 
            output_data = event.get('data', {}).get('output', {})
            if output_data: # Check if output_data is not None
                 output_messages = output_data.get('messages', [])
                 if output_messages and isinstance(output_messages[-1], AIMessage):
                    final_content = output_messages[-1].content
                    if final_content: # Only send if there's text content
                        # This is where you'd send an AG-UI `text_message` event
                        print(f"AG-UI (text_message_end): {final_content}")
            # Further event mapping for tool calls, errors, etc., would go here.

# To run this example (conceptually):
# 1. Start the FastMCP server: `python3 backend/mcp_servers/string_tools/server.py`
#    (or let mcp-use start it as a subprocess if 'command' is used in mcp_config)
# 2. Start your AG-UI Python Gateway that uses `echo_upper_agent_graph`.
# 3. Start the frontend: `npm run dev` (or similar).
```

**Running the Mini Example (Conceptual Steps):**

1.  **Start FastMCP Tool Server**: If you're not relying on `mcp-use` to start it via `command`:
    `python3 backend/mcp_servers/string_tools/server.py`
2.  **Start Agent Orchestrator Service**: This involves creating a Python web server (e.g., using FastAPI) that:
    *   Implements the AG-UI server-side protocol (handles SSE connections, message formats).
    *   Receives requests from your BFF (or directly from the AG-UI frontend if the BFF is minimal).
    *   For each request, invokes the compiled LangGraph: `echo_upper_agent_graph.astream_events(...)`.
    *   Streams the LangGraph events back to the client, formatted as AG-UI events.
3.  **Start Frontend**: Navigate to `frontend/` and run `npm run dev` (or your project's equivalent).
4.  **Interact**: Open the frontend in your browser. You should be able to send messages like "echo test" or "upper example" to the agent.

This mini-example simplifies many aspects (especially the AG-UI Python server part and BFF implementation) but illustrates the core interaction between the layers using the proposed frameworks.
